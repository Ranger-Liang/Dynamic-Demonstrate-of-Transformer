<!DOCTYPE html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer_demo</title>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.2/gsap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.2/ScrollTrigger.min.js"></script>
    <script src="js/data.js"></script>
    <script src="js/main.js"></script>
    <script src="js/station1.js"></script>
    <script src="js/station2.js"></script>
    <script src="js/station3.js"></script>
    
    <link rel="stylesheet" href="style.css">

    <!--
    作品概念：Transformer机器翻译过程的动态展示
    知识点： 抽象化、模块化、分治范式
    创新点：直观呈现复杂的复杂的数学运算、模块化展示
    关于AI：由于项目设计大量专业知识，在idea阶段大量与Gemini-2.5-flash讨论交流可视化方法并询问我需要学习的知识
           在编写第二阶段的代码时，使用Gemini-3.0-pro-preview修正了无法正确排版的bug
    参考资料：
        1. Attention is All You Need, Vaswani et al., NeurIPS 2017
        2. 注意力机制, 李沐等, 《动手学深度学习》2022年版
    -->

</head>
<body>
    
    <section id="header" class="station">
        <div class="content-wrapper">
            <h1>Dynamic Demonstrate of Transformer</h1> 
            <h2>Transformer 架构的动态展示</h2> 
            <br>
            
            <p class="desc">
                In 2017, Google published a revolutionary paper called <b><i><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank">Attention Is All You Need</a></i></b> , 
                proposing the <b>Transformer</b> architecture.
                It not only completely subverted the field of natural language processing (NLP), but also became the cornerstone of contemporary AI, driving the operation of all big models such as ChatGPT, DeepSeek, Sora, etc.
            </p>
            <p class="desc">
                This project aims to demystify the "black box" using the machine translation, a classic application of Transformer, as an example, 
                while avoiding dry and complicated mathematical formulas.
                We will offer a dynamic visualization to intuitively illustrate the inner workings of the Transformer model. 
                This approach will help you gain a deep understanding of this core AI engine and experience the immense power of the Attention.
            </p>
            
        </div>
        
       
        
        <div class="scroll-down">
            <span>⌵</span> 
            <p class="desc">Scroll down to explore</p>
        </div>
    </section>

    <section id="overview" class="station">   
        <div class="content-wrapper">
            <h2>Overview of Complete Process</h2>
            <p class="desc" id="overview-text" style="line-height: 2;">
                <b>Input:</b> Dataset including English sentences, as Source, and corresponding Chinese translations, as Target. <br>
                <b>Station 1:</b> Convert tokens into Vectors (Query, Key, Value) using projection matrices. <br>
                <b>Station 2:</b> Calculate Attention Weights by dot-producting Query with all Keys. <br>
                <b>Station 3:</b> Compute Weighted Values, while Values with higher correlation receive higher weights.  <br>
                <b>Output:</b> Produce final output through linear layers.
            <div class="image-wrapper">
                <img src="transformer demo.gif" class="image">
            </div>  
            </p>

            <div class="scroll-down">
                <span>⌵</span> 
            </div>
        </div>
    </section>
    

    <!--Station 1-->
    <section id="station-1" class="station">
        <div class="content-wrapper">
            <h2>Station 1: Linear Projections</h2>
            <p class="desc">The text (tokens) are transformed into vectors using projection matrices: 
                Chinese (Target) into <span style="color: #ef4444">Query</span> and 
                English (Source) into <span style="color: #3b82f6">Key</span> and <span style="color: #10b981">Value.</p>
            <div class="viz-container">

                <div class="pipeline-row" >
                    <span class="token">Tokens</span>
                    <div id="Matrices">Matrices</div>
                    <span class="vector">Vectors</span>
                </div>                

                <!-- Row 1: Query (红) -->
                <div class="pipeline-row" style="top: 10px" >
                    <span class="token">Target</span>
                    <div class="input-group" id="s1-in-q"></div>
                    <div class="gate">W_Q</div>
                    <div class="output-group" id="s1-out-q" ></div>
                    <span class="vector">Query</span>
                </div>

                <!-- Row 2: Key (蓝) -->
                <div class="pipeline-row" style="top: 60px">
                    <span class="token">Source</span>
                    <div class="input-group" id="s1-in-k"></div>
                    <div class="gate">W_K</div>
                    <div class="output-group" id="s1-out-k"></div>
                    <span class="vector">Key</span>
                </div>

                <!-- Row 3: Value (绿) -->
                <div class="pipeline-row" style="top: 110px">
                    <span class="token">Source</span>
                    <div class="input-group" id="s1-in-v"></div>
                    <div class="gate">W_V</div>
                    <div class="output-group" id="s1-out-v"></div>
                    <span class="vector">Value</span>
                </div>
            </div>

            <button onclick="playStation1()" class="play">▶ Play</button>
            <button onclick="reset()" class="reset">⟳ Reset</button>
          
         </div>
        
        <div class="scroll-down">
            <span>⌵</span> 
        </div>
    </section>


    <!--Station 2-->
    <section class="station">
        <div class="content-wrapper">
            <h2>Station 2: Cross Attention</h2>
            <p class="desc">The Query scan all the Keys via vector dot product to calculate Attention Weights. The higher the weight, the stronger the relevance.</p>
            <div id="container-2"></div>
        
            <button onclick="playStation2() " class="play">▶ Play</button>
            <button onclick="reset()" class="reset">⟳ Reset</button>
        </div>
        <div class="scroll-down">
            <span>⌵</span> 
        </div>
    </section>




    <!--Station 3-->
    <section id="station-3" class="station">
        <div class="content-wrapper">
            <h2>Station 3: Multiply with Value & Output</h2>
            <p class="desc">Compute Weighted Values, while Values with higher correlation receive higher weights.
                and then produce final output through linear layers.</p>
            <div class="viz-container">
            
                <div class="pipeline-row2" style="top: 20px;" >
                    <div class="row-label">Target<br>Token 1</div>
                    <div class="value-group" id= "v-wo"></div>
                    <div class="operation">➔</div>
                    <div class="result-container">
                        <div id="r1" class="result-box">我</div>
                    </div>
                </div>

                <div class="pipeline-row2" style="top: 10px;">
                    <div class="row-label">Target<br>Token 2</div>
                    <div class="value-group" id="v-zai"></div>
                    <div class="operation">➔</div>
                    <div class="result-container">
                        <div id="r2" class="result-box">在</div>
                    </div>
                </div>
                
                <div class="pipeline-row2" style="top: 10px;">
                    <div class="row-label">Target<br>Token 3</div>
                    <div class="value-group" id="v-wanda"></div>
                    <div class="operation">➔</div>
                    <div class="result-container">
                        <div id="r3" class="result-box">湾大</div>
                    </div>
                </div>

                <div class="pipeline-row2" style="top: 10px;">
                    <div class="row-label">Target<br>Token 3</div>
                    <div class="value-group" id="v-xuexi"></div>
                    <div class="operation">➔</div>
                    <div class="result-container">
                        <div id="r4" class="result-box">学习</div>
                    </div>
                </div>



            </div>
            <button onclick="playStation3()" class="play">▶ Play</button>
            <button onclick="reset()" class="reset">⟳ Reset</button>
        </div>
    </section>
    

    
    <footer class="content-wrapper">
        <p>Created by Sirui Liang, Great Bay University</p>
        <p>Sincere gratitude to Dr. Xiaodong Cun and Mr. Yihan Hu, for their patient guidance during my study of Deep Learning.</p>
    </footer>
</body>
</html>